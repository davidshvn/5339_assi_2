{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [('What are all the courses ?', 'SELECT name FROM course ;'), ('What are all the course codes ?', 'SELECT code FROM course ;')], 'dev': [('What are all the courses ?', 'SELECT name FROM course ;')], 'test': [('Please give me the names of courses .', 'SELECT name FROM course ;')]}\n",
      "{'SELECT code FROM course ;', 'SELECT name FROM course ;'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "filename = 'data.json'\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "\n",
    "    with open(filename, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    names = list(dict.fromkeys([row['data'] for row in data]))\n",
    "    unique_queries = set([row['sql'] for row in data])\n",
    "\n",
    "    dataset = {name:[] for name in names}\n",
    "    for name in names:\n",
    "        dataset[name] = [(row['question'], row['sql']) for row in data if row['data'] == name]\n",
    "\n",
    "    return dataset, unique_queries\n",
    "\n",
    "datasets, unique_queries = read_data(filename)\n",
    "print(datasets)\n",
    "print(unique_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'train': [('What are all the courses ?', 'SELECT name FROM course ;'),\n",
       "   ('What are all the course codes ?', 'SELECT code FROM course ;')],\n",
       "  'dev': [('What are all the courses ?', 'SELECT name FROM course ;')],\n",
       "  'test': [('Please give me the names of courses .',\n",
       "    'SELECT name FROM course ;')]},\n",
       " {'SELECT code FROM course ;', 'SELECT name FROM course ;'})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    {\n",
    "    'train': [\n",
    "        ('What are all the courses ?', 'SELECT name FROM course ;'),\n",
    "        ('What are all the course codes ?', 'SELECT code FROM course ;')],\n",
    "    'dev': [\n",
    "        ('What are all the courses ?', 'SELECT name FROM course ;')],\n",
    "    'test': [\n",
    "        ('Please give me the names of courses .', 'SELECT name FROM course ;')]\n",
    "    },\n",
    "    {'SELECT name FROM course ;', 'SELECT code FROM course ;'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': [\n",
    "        ('What are all the courses ?', 'SELECT name FROM course ;'),\n",
    "        ('What are all the course codes ?', 'SELECT code FROM course ;')\n",
    "    ],\n",
    "    'dev': [\n",
    "        ('What are all the courses ?', 'SELECT name FROM course ;')\n",
    "    ],\n",
    "    'test': [\n",
    "        ('Please give me the names of courses .', 'SELECT name FROM course ;'),\n",
    "        ('Please give the locations of courses .', 'SELECT location FROM course ;')\n",
    "    ]\n",
    "}\n",
    "\n",
    "labels = {\n",
    "    'SELECT name FROM course ;',\n",
    "    'SELECT code FROM course ;',\n",
    "    'SELECT location FROM course ;'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 3, 0], [0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# This is the class you need to implement\n",
    "class CodeModel:\n",
    "    def __init__(self, labels, training_data):\n",
    "        \"\"\"Prepare the class member variables.\n",
    "        Save the labels in self.labels and initialise all the weights to 0.\n",
    "\n",
    "        Keyword arguments:\n",
    "        labels -- a set of strings, each string is one SQL queryasd\n",
    "        training_data -- a list, each item is a tuple containing a question and an SQL query\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        self.labels = list(labels)\n",
    "        self.vocabulary = []\n",
    "        self.weights = []\n",
    "        for i in training_data:\n",
    "            self.vocabulary += i[0].split()\n",
    "        self.vocabulary = list(set(self.vocabulary))\n",
    "        for i in list(labels):\n",
    "            self.weights.append([0 for _ in self.vocabulary])\n",
    "    \n",
    "    def get_features(self, question, label):\n",
    "        \"\"\"Produce a list of features for a specific question and label.\n",
    "        \n",
    "        Keyword arguments:\n",
    "        question -- a string, an English question\n",
    "        label -- a string, an SQL query\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        features = [(i, label) for i in question.split()]\n",
    "        return features\n",
    "\n",
    "    def get_score(self, question, label):\n",
    "        \"\"\"Calculate the model's score for a question, label pair.\n",
    "        \n",
    "        Keyword arguments:\n",
    "        question -- a string, an English question\n",
    "        label -- a string, an SQL query\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        bow = [0 for _ in self.vocabulary]\n",
    "        counter = Counter(question.split())\n",
    "        for key, value in counter.items():\n",
    "            if key in self.vocabulary:\n",
    "                bow[self.vocabulary.index(key)] = value\n",
    "        column = self.weights[self.labels.index(label)]\n",
    "        return sum(a * b for a, b in zip(bow, column))\n",
    "\n",
    "    def update(self, question, label, change):\n",
    "        \"\"\"Modify the model.\n",
    "        Changes all weights for features for the (question, SQL query) pair by the amount indicated.\n",
    "\n",
    "        Keyword arguments:\n",
    "        question -- a string, an English question\n",
    "        label -- a string, an SQL query\n",
    "        change -- an integer, how much to change the weights\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        features = self.get_features(question, label)\n",
    "        for i, j in features:  \n",
    "            if i in self.vocabulary:\n",
    "                self.weights[self.labels.index(j)][self.vocabulary.index(i)] += change\n",
    "        return\n",
    "\n",
    "q = 'Please list courses'\n",
    "sql = 'SELECT name FROM course ;'\n",
    "model = CodeModel(labels, dataset['train'])\n",
    "model.get_features(q, sql)\n",
    "model.get_score(q, sql)\n",
    "model.update(q, sql, 1)\n",
    "model.update('the the the', sql, 1)\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT name FROM course ;'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_best_code(question, model):\n",
    "    \"\"\"Predicts the SQL for a question by using a model to try all possible labels.\n",
    "\n",
    "    Keyword arguments:\n",
    "    question -- a string, the English question\n",
    "    model -- a CodeModel, as defined in the Model question\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    scores = []\n",
    "    for label in model.labels:\n",
    "        score = model.get_score(question, label)\n",
    "        scores.append(score)\n",
    "    max_value = max(scores)\n",
    "    idx = [list(model.labels)[i] for i, num in enumerate(scores) if num == max_value]\n",
    "    idx.sort()\n",
    "    return idx[0]\n",
    "\n",
    "find_best_code(q, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, -1, -1, -1, -1, -1, 2, -1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def learn(question, answer, model, find_best_code):\n",
    "    \"\"\"Updates a model by predicting the SQL for a question and making a Perceptron update \n",
    "\n",
    "    Keyword arguments:\n",
    "    question -- a string, the English question\n",
    "    answer -- a string, the correct SQL query for this question \n",
    "    model -- a CodeModel, as defined in the Model question\n",
    "    find_best_code -- a function, the one defined the Inference question\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    pred = find_best_code(question, model)\n",
    "    if pred != answer:\n",
    "        model.update(question, answer, 1)\n",
    "        model.update(question, pred, -1)\n",
    "\n",
    "q = 'What are all the course codes ?'\n",
    "sql = 'SELECT code FROM course ;'\n",
    "learn(q, sql, model, find_best_code)\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_data = [\n",
    "    (\"What are all the courses ?\", \"SELECT name FROM course ;\"),\n",
    "    (\"What are all the course codes ?\", \"SELECT code FROM course ;\"),\n",
    "    (\"What are all the course names ?\", \"SELECT name FROM course ;\"),\n",
    "    (\"Please give me the names of courses .\", \"SELECT name FROM course ;\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('SELECT code FROM course ;', 'SELECT code FROM course ;'): 1,\n",
       " ('SELECT code FROM course ;', 'SELECT name FROM course ;'): 0,\n",
       " ('SELECT code FROM course ;', 'SELECT location FROM course ;'): 0,\n",
       " ('SELECT name FROM course ;', 'SELECT code FROM course ;'): 2,\n",
       " ('SELECT name FROM course ;', 'SELECT name FROM course ;'): 1,\n",
       " ('SELECT name FROM course ;', 'SELECT location FROM course ;'): 0,\n",
       " ('SELECT location FROM course ;', 'SELECT code FROM course ;'): 0,\n",
       " ('SELECT location FROM course ;', 'SELECT name FROM course ;'): 0,\n",
       " ('SELECT location FROM course ;', 'SELECT location FROM course ;'): 0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the function you need to implement\n",
    "def get_confusion_matrix(eval_data, model, find_best_code):\n",
    "    \"\"\"Creates a confusion matrix by predicting the SQL for a question and recording how the answer compares with the true answer \n",
    "\n",
    "    Keyword arguments:\n",
    "    eval_data -- a list of tuples containing the English question and the true SQL query\n",
    "    model -- a CodeModel, as defined in the Model question\n",
    "    find_best_code -- a function, the one defined the Inference question\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    matrix = {}\n",
    "    tuples = []\n",
    "    for question, label in eval_data:\n",
    "        pred = find_best_code(question, model)\n",
    "        tuples.append((label, pred))\n",
    "    for i in model.labels:\n",
    "        for j in model.labels:\n",
    "            matrix[(i, j)] = tuples.count((i, j))\n",
    "    return matrix\n",
    "\n",
    "matrix = get_confusion_matrix(eval_data, model, find_best_code)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the functions you need to implement\n",
    "def calculate_accuracy(confusion_matrix, labels):\n",
    "    \"\"\"Returns the accuracy based on the contents of a confusion matrix\n",
    "\n",
    "    Keyword arguments:\n",
    "    confusion_matrix -- a dictionary, as defined in the Confusion Matrix question\n",
    "    labels -- a set of strings, all the possible labels\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    count = sum(value for keys, value in confusion_matrix.items() if keys[0] == keys[1])\n",
    "    total = sum(i for i in confusion_matrix.values())\n",
    "    accuracy = count/total if total != 0 else 0 \n",
    "    return accuracy\n",
    "\n",
    "def calculate_precision(confusion_matrix, labels):\n",
    "    \"\"\"Returns a dict containing the precision for each label based on the contents of a confusion matrix\n",
    "\n",
    "    Keyword arguments:\n",
    "    confusion_matrix -- a dictionary, as defined in the Confusion Matrix question\n",
    "    labels -- a set of strings, all the possible labels\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    precisions = {}\n",
    "    for label in labels:\n",
    "        column = [(key, value) for key, value in confusion_matrix.items() if key[1] == label]\n",
    "        count = sum(value for keys, value in column if keys[0] == keys[1])\n",
    "        total = sum(value for keys, value in column)\n",
    "        precision = 1.0 if (count == 0 and total == 0) else count/total\n",
    "        precisions[label] = precision\n",
    "    return precisions \n",
    "\n",
    "def calculate_recall(confusion_matrix, labels):\n",
    "    \"\"\"Returns a dict containing the recall for each label based on the contents of a confusion matrix\n",
    "\n",
    "    Keyword arguments:\n",
    "    confusion_matrix -- a dictionary, as defined in the Confusion Matrix question\n",
    "    labels -- a set of strings, all the possible labels\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    recalls = {}\n",
    "    for label in labels:\n",
    "        column = [(key, value) for key, value in confusion_matrix.items() if key[0] == label]\n",
    "        count = sum(value for keys, value in column if keys[0] == keys[1])\n",
    "        total = sum(value for keys, value in column)\n",
    "        recall = 1.0 if (count == 0 and total == 0) else count/total\n",
    "        recalls[label] = recall\n",
    "    return recalls\n",
    "\n",
    "def calculate_macro_f1(confusion_matrix, labels):\n",
    "    \"\"\"Returns the Macro F-Score based on the contents of a confusion matrix\n",
    "\n",
    "    Keyword arguments:\n",
    "    confusion_matrix -- a dictionary, as defined in the Confusion Matrix question\n",
    "    labels -- a set of strings, all the possible labels\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    f1s = []\n",
    "    precisions = calculate_precision(confusion_matrix, labels)\n",
    "    recalls = calculate_recall(confusion_matrix, labels)\n",
    "    for p, r in zip(precisions.values(), recalls.values()):\n",
    "        f1 = 1.0 if (p+r == 0 and p*r == 0) else 2*p*r/(p+r)\n",
    "        f1s.append(f1)\n",
    "    macro_f1 = sum(f1s)/len(f1s)\n",
    "    return macro_f1\n",
    "\n",
    "calculate_accuracy(matrix, model.labels)\n",
    "calculate_precision(matrix, model.labels)\n",
    "calculate_recall(matrix, model.labels)\n",
    "calculate_macro_f1(matrix, model.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2629 229 573\n",
      "0\n",
      "{'accuracy': 0.27510917030567683, 'macro-f1': 0.5180516651104886}\n",
      "1\n",
      "{'accuracy': 0.5676855895196506, 'macro-f1': 0.6745144017202841}\n",
      "2\n",
      "{'accuracy': 0.7074235807860262, 'macro-f1': 0.7672424525365702}\n",
      "3\n",
      "{'accuracy': 0.7248908296943232, 'macro-f1': 0.7722660409192917}\n",
      "4\n",
      "{'accuracy': 0.8165938864628821, 'macro-f1': 0.8200414509238039}\n",
      "5\n",
      "{'accuracy': 0.8253275109170306, 'macro-f1': 0.8519883711060181}\n",
      "6\n",
      "{'accuracy': 0.8908296943231441, 'macro-f1': 0.8935942053589113}\n",
      "7\n",
      "{'accuracy': 0.9039301310043668, 'macro-f1': 0.909607277254336}\n",
      "8\n",
      "{'accuracy': 0.925764192139738, 'macro-f1': 0.9248443821973233}\n",
      "9\n",
      "{'accuracy': 0.9344978165938864, 'macro-f1': 0.9378073451602863}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'accuracy': 0.27510917030567683, 'macro-f1': 0.5180516651104886},\n",
       "  {'accuracy': 0.5676855895196506, 'macro-f1': 0.6745144017202841},\n",
       "  {'accuracy': 0.7074235807860262, 'macro-f1': 0.7672424525365702},\n",
       "  {'accuracy': 0.7248908296943232, 'macro-f1': 0.7722660409192917},\n",
       "  {'accuracy': 0.8165938864628821, 'macro-f1': 0.8200414509238039},\n",
       "  {'accuracy': 0.8253275109170306, 'macro-f1': 0.8519883711060181},\n",
       "  {'accuracy': 0.8908296943231441, 'macro-f1': 0.8935942053589113},\n",
       "  {'accuracy': 0.9039301310043668, 'macro-f1': 0.909607277254336},\n",
       "  {'accuracy': 0.925764192139738, 'macro-f1': 0.9248443821973233},\n",
       "  {'accuracy': 0.9344978165938864, 'macro-f1': 0.9378073451602863}],\n",
       " {'accuracy': 0.8726003490401396, 'macro-f1': 0.8473756091403151})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the function you need to implement\n",
    "def main(filename, iterations, read_data, model_maker, learn, find_best_code, get_confusion_matrix, calculate_accuracy, calculate_macro_f1):\n",
    "    \"\"\"Trains and evaluates a model on some read_data\n",
    "\n",
    "    Keyword arguments:\n",
    "    filename -- a string, the location of a json file containing data\n",
    "    iterations -- an integer, the number of iterations of training to do\n",
    "    read_data -- a function, as defined in the Data question\n",
    "    model_maker -- a class, as defined in the Model question\n",
    "    learn -- a function, as defined in the Learning question\n",
    "    find_best_code -- a function, as defined in the Inference question\n",
    "    get_confusion_matrix -- a function, as defined in the Confusion Matrix question\n",
    "    calculate_accuracy -- a function, as defined in the Evaluation Metrics question\n",
    "    calculate_macro_f1 -- a function, as defined in the Evaluation Metrics question\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    dev_scores = []\n",
    "    datasets, labels = read_data(filename)\n",
    "    print(len(datasets['train']), len(datasets['dev']), len(datasets['test']))\n",
    "    model = model_maker(labels, datasets['train'])\n",
    "    for i in range(iterations):\n",
    "        print(i)\n",
    "        j = 0\n",
    "        for question, sql in datasets['train']:\n",
    "            learn(question, sql, model, find_best_code)\n",
    "            #print(question)\n",
    "            #if j > 100:\n",
    "            #    break\n",
    "            j += 1\n",
    "        eval_matrix = get_confusion_matrix(datasets['dev'], model, find_best_code)\n",
    "        eval_accuracy = calculate_accuracy(eval_matrix, labels)\n",
    "        eval_macro_f1 = calculate_macro_f1(eval_matrix, labels)\n",
    "        dev_scores.append({'accuracy': eval_accuracy, 'macro-f1': eval_macro_f1})\n",
    "        print({'accuracy': eval_accuracy, 'macro-f1': eval_macro_f1})\n",
    "    \n",
    "    test_matrix = get_confusion_matrix(datasets['test'], model, find_best_code)\n",
    "    test_accuracy = calculate_accuracy(test_matrix, labels)\n",
    "    test_macro_f1 = calculate_macro_f1(test_matrix, labels)\n",
    "\n",
    "    return dev_scores, {'accuracy': test_accuracy, 'macro-f1': test_macro_f1}\n",
    "\n",
    "main('data_full.json', 10, read_data, CodeModel, learn, find_best_code, get_confusion_matrix, calculate_accuracy, calculate_macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "([{'accuracy': 0.29694323144104806, 'macro-f1': 0.18153134826504286},\n",
    "  {'accuracy': 0.6200873362445415, 'macro-f1': 0.364145059733295},\n",
    "  {'accuracy': 0.6593886462882096, 'macro-f1': 0.3875778089013383},\n",
    "  {'accuracy': 0.7860262008733624, 'macro-f1': 0.4653241638535756},\n",
    "  {'accuracy': 0.8165938864628821, 'macro-f1': 0.48174178762414055},\n",
    "  {'accuracy': 0.8296943231441049, 'macro-f1': 0.4951532128002716},\n",
    "  {'accuracy': 0.8820960698689956, 'macro-f1': 0.5275676937441643},\n",
    "  {'accuracy': 0.8908296943231441, 'macro-f1': 0.536221811957106},\n",
    "  {'accuracy': 0.8951965065502183, 'macro-f1': 0.5371594092182328},\n",
    "  {'accuracy': 0.8820960698689956, 'macro-f1': 0.5305845570551453}],\n",
    " {'accuracy': 0.8830715532286213, 'macro-f1': 0.7946755858520564})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
